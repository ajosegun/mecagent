# GenCAD-Code: Image-to-CAD-Code Baseline and Enhancement Report

## Introduction

This report documents the process and findings of building and evaluating an image-to-code generation system using the [CADCODER/GenCAD-Code](https://huggingface.co/datasets/CADCODER/GenCAD-Code) dataset. The primary objective was to generate valid CADQuery code from images of CAD models, establishing a baseline and then exploring enhancements within resource constraints.

---

## Dataset Handling

The GenCAD-Code dataset contains approximately 147,000 pairs of CAD images and their corresponding CADQuery code. Due to computational and storage limitations, I used a subset:

- **Training set:** 500 samples
- **Validation set:** 100 samples

This subset was selected to ensure experiments could be run efficiently on available hardware, while still providing a meaningful demonstration of the pipeline.

---

## Baseline Model

### Architecture

- **Model:** [BLIP (Bootstrapped Language-Image Pretraining)](https://huggingface.co/Salesforce/blip-image-captioning-large)
- **Approach:** Fine-tuned BLIP for image-to-code generation, treating CADQuery code as the target "caption".
- **Preprocessing:** Images were processed using BLIP's processor; code was tokenized with the BLIP tokenizer.
- **Training:** Standard Seq2Seq training loop with default hyperparameters, using the Hugging Face Trainer API.

### Evaluation Metrics

- **Valid Syntax Rate:** Percentage of generated code samples that execute without syntax errors.
- **Best IOU:** Measures geometric similarity between meshes generated by predicted and reference code.

### Results

- **Valid Syntax Rate:** 0.0
- **Mean IOU:** Error -> No CadQuery objects (Workplane, Solid, or Compound) found in script unknown

- **Eval Metrics:** {'eval_loss': 8.464953422546387, 'eval_model_preparation_time': 0.0041, 'eval_runtime': 24.9034, 'eval_samples_per_second': 4.016, 'eval_steps_per_second': 2.008}

---

## Model Enhancements

### Enhancements Applied

1. **Increased Training Epochs:**  
   The baseline was trained for 1 epoch. The enhanced model was trained for 2–3 epochs to allow better convergence.

2. **Learning Rate and Weight Decay:**  
   Lowered the learning rate to `3e-5` and added weight decay (`0.01`) to improve generalization and stability.

3. **Beam Search Decoding:**  
   Enabled beam search (`generation_num_beams=4`) during evaluation to improve the quality of generated code sequences.

4. **Gradient Accumulation:**  
   Used gradient accumulation to simulate larger batch sizes, improving training stability on limited hardware.

5. **Best Model Selection:**  
   Configured the Trainer to load the best model at the end of training based on the syntax accuracy metric.

6. **Gradient Checkpointing:**  
   Enabled gradient checkpointing to reduce memory usage and allow for larger models or batch sizes.

### Enhanced Model Results

- **Valid Syntax Rate:** 0.0
- **Mean IOU:** Error -> No CadQuery objects (Workplane, Solid, or Compound) found in script unknown

- **Eval Metrics:** {'eval_loss': 1.4290199279785156, 'eval_runtime': 27.6106, 'eval_samples_per_second': 3.622, 'eval_steps_per_second': 3.622, 'epoch': 2.0}

This run with eval_loss: 1.429 is better than the baseline with eval_loss: 8.465

---

## Choices and Rationale

- **Model Selection:**  
  BLIP was chosen for its strong performance on vision-language tasks and ease of adaptation to image-to-code generation.

- **Subset Size:**  
  The dataset was downsampled to 500/100 due to hardware and time constraints. This allowed for rapid iteration and debugging, though at the cost of model generalization.

---

## Bottlenecks and Limitations

- **Data Size:**  
  Using only a small subset limited the model's ability to generalize and likely led to overfitting.

- **Hardware Constraints:**  
  Limited GPU/CPU memory restricted batch size, model size, and training duration.

- **Model Capacity:**  
  BLIP is large; further scaling or ensembling was not feasible within resource limits.

---

## Future Enhancements (Given More Time/Resources)

1. **Full Dataset Training:**  
   Train on the entire 147K pairs to fully leverage the diversity and complexity of the dataset.

2. **Hyperparameter Tuning:**  
   Systematic search over learning rates, batch sizes, and decoding strategies.

3. **Model Architecture Exploration:**  
   Experiment with other vision-language models (e.g., BLIP-2, ViT-GPT2, or custom encoder-decoder architectures).

4. **Data Augmentation:**  
   Apply image augmentations and code perturbations to improve robustness.

5. **Error Analysis:**  
   Systematic analysis of failure cases to guide further improvements.

---

## Conclusion

Despite resource constraints, a functional baseline and enhanced model for image-to-CADQuery code generation were established.

1. The improved model was better because it has a lower eval_loss.
2. However, there was no difference on the Valid Syntax Rate, and Mean IOU.

- Further improvement
  -- Adjust tokenization for common identifiers (e.g., wp_sketch0, loop0, solid0, etc.)
  -- Change of model: BLIP’s decoder is a BERT-like model fine-tuned for text generation, but code has different distribution (symbols, structure). I believe that a code-aware language model will be better code generation.

---
